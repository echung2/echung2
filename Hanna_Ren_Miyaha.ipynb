{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIR+7KwGNCmyV7iAflDYDy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/echung2/echung2/blob/master/Hanna_Ren_Miyaha.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ginza ja_ginza\n",
        "# Install GiNZA, an open Source Japanese NLP library, which is not pre-installed on Google Colab\n",
        "# It may take several minutes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_qpWLNbuz4w",
        "outputId": "02d7283c-33c9-470f-b9fe-7e8a516749c0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ginza in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: ja_ginza in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.4.4 in /usr/local/lib/python3.10/dist-packages (from ginza) (3.7.5)\n",
            "Requirement already satisfied: plac>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from ginza) (1.4.3)\n",
            "Requirement already satisfied: SudachiPy<0.7.0,>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from ginza) (0.6.8)\n",
            "Requirement already satisfied: SudachiDict-core>=20210802 in /usr/local/lib/python3.10/dist-packages (from ginza) (20241021)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->ginza) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.4.4->ginza) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->ginza) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->ginza) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->ginza) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->ginza) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->ginza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->ginza) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.4->ginza) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.4.4->ginza) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.4.4->ginza) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ginza) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ginza) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ginza) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->ginza) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->ginza) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.4.4->ginza) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.4.4->ginza) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ginza) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ginza) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->ginza) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->ginza) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library for processing Japanese texts\n",
        "import spacy\n",
        "import ginza\n",
        "\n",
        "# Import library for topic modeling\n",
        "from gensim import corpora, models\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "\n",
        "# Import library for uploading files\n",
        "from google.colab import files\n",
        "\n",
        "# Import library for reading files\n",
        "import glob\n",
        "\n",
        "# Import library for manipulating data more conveniently\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LFoLmRdNvJsQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from google.colab import files\n",
        "import re"
      ],
      "metadata": {
        "id": "SFO4TCJR3ZKk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Read data**"
      ],
      "metadata": {
        "id": "DJR0Of0lx4Eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload the data on Google Colab\n",
        "\n",
        "you can upload the file by clicking the folder-shaped icon (\"Files\") to open the file browser on the left side of the notebook. Click the upload button that appears at the top of the file browser (the up arrow button)."
      ],
      "metadata": {
        "id": "6FCN2qEox_cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "S2k3MC9xvVxr",
        "outputId": "6825a271-79b9-4010-fb8d-a4db22c5873c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4e6bb1a7-0a72-452b-bdc6-c33f9a912b2a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4e6bb1a7-0a72-452b-bdc6-c33f9a912b2a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving H_Miyaha_OCR.txt to H_Miyaha_OCR (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: Tokenize article text**"
      ],
      "metadata": {
        "id": "6NZ3RPn6xcCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text from uploaded file\n",
        "file_name = list(uploaded.keys())[0]\n",
        "with open(file_name, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "8roWuEnd3olr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text: remove special characters\n",
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "text = re.sub(r'\\s+', ' ', text).strip()"
      ],
      "metadata": {
        "id": "ThZQ0r293ycD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Japanese stopwords\n",
        "stopwords = set([\n",
        "    'は', 'が', 'の', 'に', 'を', 'へ', 'と', 'で', 'や', 'も', 'から', 'まで', 'だけ', 'より', 'しか', 'ながら', 'など',\n",
        "    '私', '僕', '俺', 'あなた', '君', '彼', '彼女', '私たち', 'あなたたち', '皆', 'これ', 'それ', 'あれ', 'どれ', 'ここ',\n",
        "    'そこ', 'あそこ', 'どこ', 'とても', 'かなり', '少し', 'ちょっと', '大体', 'ほとんど', 'また', 'なぜ', 'どう', '何', 'いつ',\n",
        "    'どこ', 'ただ', 'ようやく', 'やはり', 'さらに', 'すぐに', 'いる', 'ある', 'する', 'なる', '行く', '来る', '見る', '聞く',\n",
        "    '言う', '思う', '感じる', '分かる', '知る', '良い', '悪い', '大きい', '小さい', '高い', '低い', '多い', '少ない', '簡単だ',\n",
        "    '難しい', '長い', '短い', 'そして', 'しかし', 'それでも', 'だから', 'または', 'それに', 'つまり', 'なぜなら', 'ああ', 'うん',\n",
        "    'ええ', 'はい', 'いいえ', 'おお', 'あれ', 'まあ', 'えっ', 'あっ', 'ふうん', 'こと', 'もの', 'ところ', 'よう', '例え',\n",
        "    '例えば', '場合', 'ような', '様子', 'する', 'ある', 'いる', 'なる', 'れる', 'の', 'も', 'た', 'が', 'に', 'は', 'を', 'で', 'Page'\n",
        "])"
      ],
      "metadata": {
        "id": "KJtpDNBD4ygz"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the \"ja_ginza\" SpaCy pipeline\n",
        "try:\n",
        "    nlp = spacy.load(\"ja_ginza\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ja_ginza: {e}\")"
      ],
      "metadata": {
        "id": "CHpTajAK32Gr"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split text into chunks if necessary\n",
        "chunk_size = 5000\n",
        "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]"
      ],
      "metadata": {
        "id": "Owz2WoUb3909"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and filter words\n",
        "tokens = []\n",
        "for chunk in chunks:\n",
        "    doc = nlp(chunk)\n",
        "    for token in doc:\n",
        "        if token.pos_ in ['NOUN', 'VERB', 'ADJ']:\n",
        "            tokens.append(token.text)"
      ],
      "metadata": {
        "id": "sWNrCShh4D1B"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display extracted tokens for verification (optional)\n",
        "print(f\"Filtered Tokens: {tokens[:50]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVQzijyX5Aso",
        "outputId": "e827f954-682c-4739-b02f-30efb9b52293"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens: ['Page', '羽', '贈る', '拳銃', 'Page', 'Page', '羽', '贈る', '拳銃', '手', '中', '銃', 'ある', 'つめたく', '光る', '黒色', '銃', 'きみ', '小さな', '手のひら', '慣れ', '真', '黒', '重み', '戸惑い', '汗', 'いる', '真夜中', '忍び込ん', '書斎', '無数', '傷', '刻ま', '年', '代物', '机', '色褪せ', '木目', '怪物', '眼球', '怯え', '机', '引き出し', '刃先', '錆び', 'IMRI', '用', '接続', 'プラグ', 'レントゲン']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary and corpus for LDA\n",
        "dictionary = corpora.Dictionary([tokens])\n",
        "corpus = [dictionary.doc2bow(tokens)]"
      ],
      "metadata": {
        "id": "Y2ImdKGf5ERR"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LDA for topic modeling with additional parameters\n",
        "num_topics = 20  # Specify the number of topics\n",
        "lda_model = gensim.models.ldamodel.LdaModel(\n",
        "    corpus=corpus,\n",
        "    num_topics=num_topics,\n",
        "    id2word=dictionary,  # Use the dictionary created above\n",
        "    passes=20,  # Increase passes for better convergence\n",
        "    iterations=400,  # Increase iterations for better optimization\n",
        "    random_state=50  # For reproducibility\n",
        ")\n",
        "\n",
        "# Display topics\n",
        "for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=10):\n",
        "    print(f\"Topic {idx + 1}: {topic}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSzW-sYLEkkp",
        "outputId": "c64a6c04-2642-4c10-c9d8-ad49acbe4fe6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: 0.000*\"い\" + 0.000*\"継\" + 0.000*\"実\" + 0.000*\"羽\" + 0.000*\"こと\" + 0.000*\"いる\" + 0.000*\"神\" + 0.000*\"し\" + 0.000*\"脳\" + 0.000*\"インプラント\"\n",
            "Topic 2: 0.017*\"い\" + 0.017*\"こと\" + 0.017*\"羽\" + 0.015*\"継\" + 0.014*\"実\" + 0.013*\"いる\" + 0.010*\"神\" + 0.009*\"脳\" + 0.009*\"し\" + 0.007*\"自分\"\n",
            "Topic 3: 0.000*\"羽\" + 0.000*\"いる\" + 0.000*\"こと\" + 0.000*\"実\" + 0.000*\"い\" + 0.000*\"神\" + 0.000*\"継\" + 0.000*\"脳\" + 0.000*\"し\" + 0.000*\"インプラント\"\n",
            "Topic 4: 0.000*\"漆黒\" + 0.000*\"滾々\" + 0.000*\"演出\" + 0.000*\"演じ\" + 0.000*\"漏れ聞こえ\" + 0.000*\"漏らす\" + 0.000*\"漏らし\" + 0.000*\"潤し\" + 0.000*\"滞り\" + 0.000*\"滞っ\"\n",
            "Topic 5: 0.000*\"漆黒\" + 0.000*\"滾々\" + 0.000*\"演出\" + 0.000*\"演じ\" + 0.000*\"漏れ聞こえ\" + 0.000*\"漏らす\" + 0.000*\"漏らし\" + 0.000*\"潤し\" + 0.000*\"滞り\" + 0.000*\"滞っ\"\n",
            "Topic 6: 0.000*\"い\" + 0.000*\"羽\" + 0.000*\"いる\" + 0.000*\"継\" + 0.000*\"こと\" + 0.000*\"実\" + 0.000*\"脳\" + 0.000*\"神\" + 0.000*\"し\" + 0.000*\"自分\"\n",
            "Topic 7: 0.000*\"漆黒\" + 0.000*\"滾々\" + 0.000*\"演出\" + 0.000*\"演じ\" + 0.000*\"漏れ聞こえ\" + 0.000*\"漏らす\" + 0.000*\"漏らし\" + 0.000*\"潤し\" + 0.000*\"滞り\" + 0.000*\"滞っ\"\n",
            "Topic 8: 0.000*\"こと\" + 0.000*\"羽\" + 0.000*\"い\" + 0.000*\"実\" + 0.000*\"神\" + 0.000*\"いる\" + 0.000*\"し\" + 0.000*\"継\" + 0.000*\"いう\" + 0.000*\"脳\"\n",
            "Topic 9: 0.000*\"羽\" + 0.000*\"継\" + 0.000*\"こと\" + 0.000*\"い\" + 0.000*\"神\" + 0.000*\"実\" + 0.000*\"脳\" + 0.000*\"いる\" + 0.000*\"し\" + 0.000*\"自分\"\n",
            "Topic 10: 0.000*\"こと\" + 0.000*\"実\" + 0.000*\"羽\" + 0.000*\"継\" + 0.000*\"い\" + 0.000*\"神\" + 0.000*\"いる\" + 0.000*\"脳\" + 0.000*\"し\" + 0.000*\"インプラント\"\n",
            "Topic 11: 0.000*\"羽\" + 0.000*\"い\" + 0.000*\"こと\" + 0.000*\"実\" + 0.000*\"いる\" + 0.000*\"神\" + 0.000*\"継\" + 0.000*\"し\" + 0.000*\"脳\" + 0.000*\"いう\"\n",
            "Topic 12: 0.000*\"羽\" + 0.000*\"い\" + 0.000*\"継\" + 0.000*\"いる\" + 0.000*\"こと\" + 0.000*\"実\" + 0.000*\"脳\" + 0.000*\"いう\" + 0.000*\"自分\" + 0.000*\"神\"\n",
            "Topic 13: 0.000*\"継\" + 0.000*\"い\" + 0.000*\"こと\" + 0.000*\"羽\" + 0.000*\"いる\" + 0.000*\"実\" + 0.000*\"脳\" + 0.000*\"神\" + 0.000*\"し\" + 0.000*\"インプラント\"\n",
            "Topic 14: 0.000*\"羽\" + 0.000*\"こと\" + 0.000*\"い\" + 0.000*\"継\" + 0.000*\"実\" + 0.000*\"し\" + 0.000*\"いる\" + 0.000*\"神\" + 0.000*\"ある\" + 0.000*\"脳\"\n",
            "Topic 15: 0.000*\"漆黒\" + 0.000*\"滾々\" + 0.000*\"演出\" + 0.000*\"演じ\" + 0.000*\"漏れ聞こえ\" + 0.000*\"漏らす\" + 0.000*\"漏らし\" + 0.000*\"潤し\" + 0.000*\"滞り\" + 0.000*\"滞っ\"\n",
            "Topic 16: 0.000*\"漆黒\" + 0.000*\"滾々\" + 0.000*\"演出\" + 0.000*\"演じ\" + 0.000*\"漏れ聞こえ\" + 0.000*\"漏らす\" + 0.000*\"漏らし\" + 0.000*\"潤し\" + 0.000*\"滞り\" + 0.000*\"滞っ\"\n",
            "Topic 17: 0.000*\"漆黒\" + 0.000*\"滾々\" + 0.000*\"演出\" + 0.000*\"演じ\" + 0.000*\"漏れ聞こえ\" + 0.000*\"漏らす\" + 0.000*\"漏らし\" + 0.000*\"潤し\" + 0.000*\"滞り\" + 0.000*\"滞っ\"\n",
            "Topic 18: 0.000*\"漆黒\" + 0.000*\"滾々\" + 0.000*\"演出\" + 0.000*\"演じ\" + 0.000*\"漏れ聞こえ\" + 0.000*\"漏らす\" + 0.000*\"漏らし\" + 0.000*\"潤し\" + 0.000*\"滞り\" + 0.000*\"滞っ\"\n",
            "Topic 19: 0.000*\"漆黒\" + 0.000*\"滾々\" + 0.000*\"演出\" + 0.000*\"演じ\" + 0.000*\"漏れ聞こえ\" + 0.000*\"漏らす\" + 0.000*\"漏らし\" + 0.000*\"潤し\" + 0.000*\"滞り\" + 0.000*\"滞っ\"\n",
            "Topic 20: 0.000*\"漆黒\" + 0.000*\"滾々\" + 0.000*\"演出\" + 0.000*\"演じ\" + 0.000*\"漏れ聞こえ\" + 0.000*\"漏らす\" + 0.000*\"漏らし\" + 0.000*\"潤し\" + 0.000*\"滞り\" + 0.000*\"滞っ\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LDA for topic modeling\n",
        "#** num_topics = 10  # Specify the number of topics\n",
        "#** lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=50)"
      ],
      "metadata": {
        "id": "jStk9kpw5H89"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this process, you'll use SpaCy, a natural language processing library, and GiNZA, a Japanese NLP library that works on spaCy.\n",
        "\n",
        "이 과정에서는 자연어 처리 라이브러리인 SpaCy와 spaCy에서 작동하는 일본어 NLP 라이브러리인 GiNZA를 사용하게 됩니다."
      ],
      "metadata": {
        "id": "_ip8El6GxmnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Extract topics and words with probabilities\n",
        "topics = lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "xKyeQ-3DFgb6"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for CSV\n",
        "csv_data = []\n",
        "for topic_idx, topic_words in topics:\n",
        "    topic_row = [f\"Topic {topic_idx + 1}\"]\n",
        "    for word, prob in topic_words:\n",
        "        topic_row.append(f\"{word} ({prob:.4f})\")\n",
        "    csv_data.append(topic_row)\n"
      ],
      "metadata": {
        "id": "t6eywU1DxPM4"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Extract topics and words with probabilities\n",
        "topics = lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "7QskFEe4vXNf"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for CSV\n",
        "csv_data = []\n",
        "for topic_idx, topic_words in topics:\n",
        "    topic_row = [f\"Topic {topic_idx + 1}\"]\n",
        "    for word, prob in topic_words:\n",
        "        topic_row.append(f\"{word} ({prob:.4f})\")\n",
        "    csv_data.append(topic_row)\n",
        "\n"
      ],
      "metadata": {
        "id": "ngZNQKdcH-7b"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the topics to a CSV file with UTF-8 with BOM encoding\n",
        "output_file = \"output-1.csv\"\n",
        "with open(output_file, mode='w', encoding='utf-8-sig', newline='') as file:  # 'utf-8-sig' to add BOM\n",
        "    writer = csv.writer(file)\n",
        "    # Write header\n",
        "    writer.writerow([\"Topic\", \"Words with Probabilities\"])\n",
        "    # Write topics and words\n",
        "    for row in csv_data:\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"Results saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xeil-tM9IBzg",
        "outputId": "d44aa0e6-1668-4eaf-bf6c-d39bf025a04e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to output-1.csv\n"
          ]
        }
      ]
    }
  ]
}