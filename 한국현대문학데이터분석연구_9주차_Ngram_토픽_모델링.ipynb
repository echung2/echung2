{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/echung2/echung2/blob/master/%ED%95%9C%EA%B5%AD%ED%98%84%EB%8C%80%EB%AC%B8%ED%95%99%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EC%97%B0%EA%B5%AC_9%EC%A3%BC%EC%B0%A8_Ngram_%ED%86%A0%ED%94%BD_%EB%AA%A8%EB%8D%B8%EB%A7%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m44GABGZt-o"
      },
      "source": [
        "# Ngram / Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypvbfJ0IG3qy"
      },
      "source": [
        "# 나눔고딕\n",
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum* -qq\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "# 패키지 설치\n",
        "!pip install -U gensim kiwipiepy tomotopy nltk flashtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qp6I8X7Hi8-"
      },
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from kiwipiepy import Kiwi, Option\n",
        "kiwi = Kiwi()\n",
        "kiwi.prepare()\n",
        "\n",
        "import tomotopy as tp\n",
        "import sys\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "\n",
        "import itertools\n",
        "from collections import Counter\n",
        "\n",
        "import regex #확장된 정규표현식. 일반 정규표현식은 import re\n",
        "import nltk\n",
        "from nltk import collocations\n",
        "\n",
        "from flashtext import KeywordProcessor\n",
        "kp = KeywordProcessor()\n",
        "\n",
        "from gensim.models.phrases import Phrases, Phraser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9oD7EiqZtFb"
      },
      "source": [
        "# 이인직 소설 자료 다운로드\n",
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1AY763FcPXN_iBo_sVMXHugQ8UHFvb_nN' -O lee.xlsx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPMCcAbJcc6L"
      },
      "source": [
        "df = pd.read_excel('lee.xlsx')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcJO9BZ1dpOG"
      },
      "source": [
        "##### 형태소 분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co9vHhcaHVuS"
      },
      "source": [
        "# 몇가지 품사 제외한 모든 품사 추출 + 품사 태그 포함\n",
        "def tokenize_tag(sent):\n",
        "    res, score = kiwi.analyze(sent)[0] # 첫번째 결과를 사용\n",
        "    return [word + ('다' if tag.startswith('V') else '')+ '/'+ tag # 동사/형용사에는 '다'를 붙여줌 + / 품사\n",
        "            for word, tag, _, _ in res\n",
        "            if not tag.startswith('E') and not tag.startswith('J') and not tag.startswith('S')] # 조사, 어미, 특수기호 및 stopwords에 포함된 단어는 제거"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX147bkNom6H"
      },
      "source": [
        "# 조사, 어미, 특수기호 제외한 토크나이징\n",
        "df['tokens'] = df['paragraph'].progress_map(lambda x:tokenize_tag(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czFtfkcUp3s4"
      },
      "source": [
        " # 불용어 리스트\n",
        "stopwords = set(['이다/VCP','하다/VV','하다/VX','위하다/VV','되다/VV','있다/VV', '있다/VX','없다/VA','않다/VX','있다/VV','아니하다/VX'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JkxUvM4qAKL"
      },
      "source": [
        "# 불용어 제거\n",
        "df['tokens'] = df['tokens'].map(lambda x:[w for w in x if not w in set(stopwords)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxe386hL3b1n"
      },
      "source": [
        "df['tokens'][233]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkBf2EW73T4k"
      },
      "source": [
        "# 토큰 없는 행 삭제\n",
        "df[df['tokens'].map(lambda x:len(x)==0)]\n",
        "df = df[df['tokens'].map(lambda x:len(x)!=0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5dNmODE3xgy"
      },
      "source": [
        "# reset index\n",
        "df = df.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9irB8jPmBy9"
      },
      "source": [
        "# 모든 품사 + 태그\n",
        "def tokenize_all_tag(sent):\n",
        "    res, score = kiwi.analyze(sent)[0] # 첫번째 결과를 사용\n",
        "    return [word + ('다' if tag.startswith('V') else '')+ '/'+ tag # 동사/형용사에는 '다'를 붙여줌 + / 품사\n",
        "            for word, tag, _, _ in res]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkSIJEt0dhb1"
      },
      "source": [
        "# 모든 형태소 포함한 토크나이징\n",
        "df['tokens_all'] = df['paragraph'].progress_map(lambda x:tokenize_all_tag(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxAdxZvhijkH"
      },
      "source": [
        "token_list = list(itertools.chain(*df['tokens'].tolist()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJiINum-dsuV"
      },
      "source": [
        "### 연어(Collocation) - nltk 활용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6SSvuIAeZqC"
      },
      "source": [
        "##### Bigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoEuzlwnfWsl"
      },
      "source": [
        "bi_measures = nltk.collocations.BigramAssocMeasures()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IL2GXLSdzwU"
      },
      "source": [
        "finder = collocations.BigramCollocationFinder.from_words(token_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KWohvFKfDub"
      },
      "source": [
        "finder.apply_freq_filter(10) # n회 이상 등장한 bigram만 뽑기\n",
        "finder.nbest(bi_measures.pmi, 20) # 상위 n개"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CEIAkhZHBJ5"
      },
      "source": [
        "### 학습 기반 Ngram 찾기\n",
        "https://lovit.github.io/nlp/2018/10/23/ngram/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EziMbOTmjN9R"
      },
      "source": [
        "##### get_ngram_counter (학습데이터 만들기)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBuhIAjwIZoz"
      },
      "source": [
        "def get_ngram_counter(docs, min_count, n_range=(1,3)):\n",
        "\n",
        "    def to_ngrams(words, n):\n",
        "        ngrams = []\n",
        "        for b in range(0, len(words) - n + 1):\n",
        "            ngrams.append(tuple(words[b:b+n]))\n",
        "        return ngrams\n",
        "\n",
        "    n_begin, n_end = n_range\n",
        "    ngram_counter = defaultdict(int)\n",
        "    for doc in docs:\n",
        "        words = tokenize_tag(doc)\n",
        "        for n in range(n_begin, n_end + 1):\n",
        "            for ngram in to_ngrams(words, n):\n",
        "                ngram_counter[ngram] += 1\n",
        "\n",
        "    ngram_counter = {\n",
        "        ngram:count for ngram, count in ngram_counter.items()\n",
        "        if count >= min_count\n",
        "    }\n",
        "\n",
        "    return ngram_counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJEmB9F2JUjD"
      },
      "source": [
        "# 학습 방식\n",
        "get_ngram_counter(['조니 뎁','조니 뎁','4차 산업 혁명','산업혁명'],2) #n회 이상 등장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmM0SSJUiOC0"
      },
      "source": [
        "ngram_counter = get_ngram_counter(token_list,10) # 10 회 이상 등장한 ngram으로 학습"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULjR92VjjHql"
      },
      "source": [
        "# 확인\n",
        "sorted(ngram_counter, key=lambda x:-ngram_counter[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaE7FmFrjRf4"
      },
      "source": [
        "##### get_ngram_counter 기반으로 Ngram 형태소 분석기 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdZHLGkOM1UU"
      },
      "source": [
        "class NgramTokenizer:\n",
        "\n",
        "    def __init__(self, ngrams, base_tokenizer, n_range=(1, 3)):\n",
        "        self.ngrams = ngrams\n",
        "        self.base_tokenizer = base_tokenizer\n",
        "        self.n_range = n_range\n",
        "\n",
        "    def __call__(self, sent):\n",
        "        return self.tokenize(sent)\n",
        "\n",
        "    def tokenize(self, sent):\n",
        "        if not sent:\n",
        "            return []\n",
        "\n",
        "        unigrams = self.base_tokenizer(sent)\n",
        "\n",
        "        n_begin, n_end = self.n_range\n",
        "        ngrams = []\n",
        "        for n in range(n_begin, n_end + 1):\n",
        "            for ngram in self._to_ngrams(unigrams, n):\n",
        "                ngrams.append('-'.join(ngram))\n",
        "        return ngrams\n",
        "\n",
        "    def _to_ngrams(self, words, n):\n",
        "        ngrams = []\n",
        "        for b in range(0, len(words) - n + 1):\n",
        "            ngram = tuple(words[b:b+n])\n",
        "            if ngram in self.ngrams:\n",
        "                ngrams.append(ngram)\n",
        "        return ngrams\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf71PIwENCU2"
      },
      "source": [
        "ngram_tokenizer = NgramTokenizer(ngram_counter, tokenize_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qT3S9Ysjo4N"
      },
      "source": [
        "tokenize_tag('옥련은 겁이 나다')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOCfZC6kOzTG"
      },
      "source": [
        "# 기존 형태소 분석기와 비교\n",
        "ngram_tokenizer('옥단은 겁이 나다')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfwMt-LZ1Djr"
      },
      "source": [
        "df['ngram_tokens'] = df['paragraph'].progress_map(lambda x:ngram_tokenizer(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMF8pCOV2BIK"
      },
      "source": [
        "df['ngram_tokens'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rHdpeZT8aOy"
      },
      "source": [
        "df['ngram_tokens'].map(lambda x:len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54FAQzr9kZm6"
      },
      "source": [
        "##### ngram 학습 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ75-HGSjzzF"
      },
      "source": [
        "vectorizer = CountVectorizer(\n",
        "    tokenizer = ngram_tokenizer,\n",
        "    lowercase = False,\n",
        ")\n",
        "x = vectorizer.fit_transform(token_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOemTm6Wkkex"
      },
      "source": [
        "for ngram, count in sorted(ngram_counter.items(), key=lambda x:-x[1]):\n",
        "    if '겁/NNG' in ngram: # 'OO'을 포함하는 bigram\n",
        "        print(ngram, count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmkzmLdTkCbQ"
      },
      "source": [
        "##### ngram_score로 고도화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRV2KVn2QB0v"
      },
      "source": [
        "def get_ngram_score(ngram_counter, delta=20): #delta : n회 이상 등장한 경우만 점수 계산\n",
        "    ngrams_ = {}\n",
        "    for ngram, count in ngram_counter.items():\n",
        "        if len(ngram) == 1:\n",
        "            continue\n",
        "        first = ngram_counter[ngram[:-1]]\n",
        "        second = ngram_counter[ngram[1:]]\n",
        "        score = (count - delta) / (first * second)\n",
        "        if score > 0:\n",
        "            ngrams_[ngram] = (count, score)\n",
        "    return ngrams_\n",
        "\n",
        "ngram_scores = get_ngram_score(ngram_counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAAZBRZZm8su"
      },
      "source": [
        "sorted(ngram_scores.items(), key=lambda x:-x[1][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74w7BWLvoKlj"
      },
      "source": [
        "# trigram 확인\n",
        "trigram_scores = {\n",
        "    ngram:score for ngram, score in ngram_scores.items()\n",
        "    if len(ngram) == 3\n",
        "}\n",
        "\n",
        "sorted(trigram_scores.items(), key=lambda x:-x[1][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hagm7N3CoXvZ"
      },
      "source": [
        "### Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GldbTQUpsDSk"
      },
      "source": [
        "##### 1. 신문기사 텍스트 분석 예시 (LDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szQ9n6IevB6p"
      },
      "source": [
        "# 빅카인즈 뉴스 데이터 가져오기\n",
        "# https://drive.google.com/file/d/15_N55LQM1HsjfwmlM6VWPo2stN9DmWoX/view?usp=sharing\n",
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=15_N55LQM1HsjfwmlM6VWPo2stN9DmWoX' -O young_2017_2019.xlsx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ii05Fa9vXbg"
      },
      "source": [
        "# 2017 ~ 2019년에 발간된 '청년' 관련 기사 (검색어 : 청년)\n",
        "young = pd.read_excel(\"young_2017_2019.xlsx\")\n",
        "young.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWF-aaH_vYXY"
      },
      "source": [
        "young = young[young['분석제외 여부'].isna()] #결측이 있는 행만 살리기 (dropna의 반대)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGuFRlEIvZR9"
      },
      "source": [
        "# 본문 없는 기사 삭제\n",
        "young = young.dropna(subset=['본문'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNTCh4LPvaPS"
      },
      "source": [
        "# 중복 기사 삭제 (제목 및 본문 기준)\n",
        "young = young.drop_duplicates(subset=['제목','본문'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv9g1maPvcJT"
      },
      "source": [
        "#index 재설정, drop은 기존 index 삭제\n",
        "young = young.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp6rhpbpwqch"
      },
      "source": [
        "young['키워드'] = young['키워드'].str.split(',')\n",
        "young['키워드']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBHcrQ5nvdfJ"
      },
      "source": [
        "# 모델 초기화\n",
        "LDA = tp.LDAModel(k=10,min_df=100,tw=tp.TermWeight.IDF,rm_top=3, seed=2021) # Hyperparameter 숙지 필수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZXUYgsBvp8W"
      },
      "source": [
        "for token in tqdm(young['키워드'].tolist()):\n",
        "    LDA.add_doc(token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQEDjp6qv0lR"
      },
      "source": [
        "# 한번에 20회씩 총 500회 학습\n",
        "print('Num docs:', len(LDA.docs), ', Vocab size:', LDA.num_vocabs, ', Num words:', LDA.num_words)\n",
        "print('Removed top words:', LDA.removed_top_words)\n",
        "print('Training...', file=sys.stderr, flush=True)\n",
        "for i in range(0, 500, 20):\n",
        "    LDA.train(20)\n",
        "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, LDA.ll_per_word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeRNFR40wLiN"
      },
      "source": [
        "# 학습 결과\n",
        "LDA.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcipInF1wMrA"
      },
      "source": [
        "# 토픽별 상위 단어 N개 확인\n",
        "for i in range(LDA.k):\n",
        "    res = LDA.get_topic_words(i, top_n=10) # top 10\n",
        "    print('Topic #{}'.format(i), end='\\t')\n",
        "    print(', '.join(w for w, p in res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgA3181_xhXO"
      },
      "source": [
        "##### 2. 신문기사 텍스트 분석 예시 (DMR)\n",
        "\"언론사 성향에 따른 청년 담론의 차이\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94dK9Q9Ax3j7"
      },
      "source": [
        "young['언론사'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7fYDODrx8q5"
      },
      "source": [
        "# 언론사_성향 컬럼 추가\n",
        "young['언론사_성향'] = pd.Series()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBStJ5Tfx9wb"
      },
      "source": [
        "young['언론사_성향'][young['언론사'].isin(['조선일보','중앙일보','동아일보'])] = '보수'\n",
        "young['언론사_성향'][young['언론사'].isin(['경향신문','한겨레'])] = '진보'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wsIMOZGyApQ"
      },
      "source": [
        "young['언론사_성향'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud9ON83PyJT4"
      },
      "source": [
        "DMR = tp.DMRModel(k=10,min_df=100,tw=tp.TermWeight.PMI,rm_top=3, seed=2021)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4SEKN9CyRUj"
      },
      "source": [
        "token_politics_dict = young[['키워드','언론사_성향']].to_dict('index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw05JnDBySbF"
      },
      "source": [
        "for k in tqdm(token_politics_dict.keys()):\n",
        "    DMR.add_doc(token_politics_dict[k]['키워드'],token_politics_dict[k]['언론사_성향'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf9LnxVByTVq"
      },
      "source": [
        "# 학습 준비\n",
        "DMR.train(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XlmKj_AyYvA"
      },
      "source": [
        "# 한번에 20회씩 총 500회 학습\n",
        "print('Num docs:', len(DMR.docs), ', Vocab size:', DMR.num_vocabs, ', Num words:', DMR.num_words)\n",
        "print('Removed top words:', DMR.removed_top_words)\n",
        "print('Training...', file=sys.stderr, flush=True)\n",
        "for i in range(0, 500, 20):\n",
        "    DMR.train(20)\n",
        "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, DMR.ll_per_word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poFYpd39yZwc"
      },
      "source": [
        "DMR.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw8JpswLycFZ"
      },
      "source": [
        "for i in range(DMR.k):\n",
        "    res = DMR.get_topic_words(i, top_n=10)\n",
        "    print('Topic #{}'.format(i), end='\\t')\n",
        "    print(', '.join(w for w, p in res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfT72AUWyeT5"
      },
      "source": [
        "DMR.metadata_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isreFl_cyfZ4"
      },
      "source": [
        "# calculate topic distribution for each metadata using softmax\n",
        "probs = np.exp(DMR.lambdas - DMR.lambdas.max(axis=0))\n",
        "probs /= probs.sum(axis=0)\n",
        "\n",
        "print('명목변수 별 토픽 비율')\n",
        "for f, metadata_name in enumerate(DMR.metadata_dict):\n",
        "    print(metadata_name, probs[:, f], '\\n')\n",
        "\n",
        "x = np.arange(DMR.k)\n",
        "width = 1 / (DMR.f + 2)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "for f, metadata_name in enumerate(DMR.metadata_dict):\n",
        "    ax.bar(x + width * (f - DMR.f / 2), probs[:, f], width, label=DMR.metadata_dict[f])\n",
        "\n",
        "ax.set_ylabel('Probabilities')\n",
        "ax.set_yscale('log')\n",
        "ax.set_title('명목변수에 따른 토픽 비율')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(['Topic #{}'.format(k) for k in range(DMR.k)],rotation=45) # x축 라벨 45도로 회전\n",
        "ax.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcprUyaOyjF3"
      },
      "source": [
        "##### 이인직 작품별 토픽 차이(DMR)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4sY2tFmymV8"
      },
      "source": [
        "df['title'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oxS1dHfzyMB"
      },
      "source": [
        "# bigram으로 만들기\n",
        "def make_bigram(sents):\n",
        "    bigram = Phrases(sents,min_count=50) # n회 이상 나온 단어만 bigram\n",
        "    bigram_mod = Phraser(bigram)\n",
        "    sents_bigram = [bigram_mod[doc] for doc in sents]\n",
        "    return sents_bigram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gtM63Ys0KDv"
      },
      "source": [
        "sents_bigram = make_bigram(df['tokens'].to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOqeMiMH0PGH"
      },
      "source": [
        "# bigram 단어 확인\n",
        "bigram_list = [word for sent in sents_bigram for word in sent if \"_\" in word]\n",
        "Counter(bigram_list).most_common(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRy4f3UZ6i93"
      },
      "source": [
        "df['bigram'] = pd.Series(sents_bigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX9pjRVH7CQZ"
      },
      "source": [
        "# 1음절 단어 삭제\n",
        "hangul_1 = regex.compile(r'^\\p{Hangul}{1}$')\n",
        "hangul_1_except = regex.compile(r'^(?!겁/NNG|돈/NNG|꿈/NNG|산/NNG)\\p{Hangul}{1}/\\w+$')\n",
        "df['bigram'] = df['bigram'].map(lambda x:[w for w in x if not hangul_1_except.match(w)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tl3bvpd7tW3"
      },
      "source": [
        "# 빈 토큰이 있는 행 삭제\n",
        "df = df[df['bigram'].map(lambda x:len(x)!=0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2U32xAl2klJ"
      },
      "source": [
        "token_title_dict = df[['bigram','title']].to_dict('index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBOXF6zE0S5z"
      },
      "source": [
        "# 모델 초기화\n",
        "DMR_lee = tp.DMRModel(k=10,min_df=10,tw=tp.TermWeight.PMI, seed=2021)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-tUd_Ah2sEz"
      },
      "source": [
        "for k in tqdm(token_title_dict.keys()):\n",
        "    DMR_lee.add_doc(token_title_dict[k]['bigram'],token_title_dict[k]['title'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3zzhPdA2wuV"
      },
      "source": [
        "# 한번에 20회씩 총 500회 학습\n",
        "print('Num docs:', len(DMR_lee.docs), ', Vocab size:', DMR_lee.num_vocabs, ', Num words:', DMR_lee.num_words)\n",
        "print('Removed top words:', DMR_lee.removed_top_words)\n",
        "print('Training...', file=sys.stderr, flush=True)\n",
        "for i in range(0, 500, 20):\n",
        "    DMR_lee.train(20)\n",
        "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, DMR_lee.ll_per_word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTSHpm7x4CQS"
      },
      "source": [
        "DMR_lee.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX630h8H4RZC"
      },
      "source": [
        "for i in range(DMR_lee.k):\n",
        "    res = DMR_lee.get_topic_words(i, top_n=10)\n",
        "    print('Topic #{}'.format(i), end='\\t')\n",
        "    print(', '.join(w for w, p in res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J55weLl4wg1"
      },
      "source": [
        "DMR_lee.metadata_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKyHbwtZ437o"
      },
      "source": [
        "# calculate topic distribution for each metadata using softmax\n",
        "probs = np.exp(DMR_lee.lambdas - DMR_lee.lambdas.max(axis=0))\n",
        "probs /= probs.sum(axis=0)\n",
        "\n",
        "print('명목변수 별 토픽 비율')\n",
        "for f, metadata_name in enumerate(DMR_lee.metadata_dict):\n",
        "    print(metadata_name, probs[:, f], '\\n')\n",
        "\n",
        "x = np.arange(DMR_lee.k)\n",
        "width = 1 / (DMR_lee.f + 2)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "for f, metadata_name in enumerate(DMR_lee.metadata_dict):\n",
        "    ax.bar(x + width * (f - DMR_lee.f / 2), probs[:, f], width, label=DMR_lee.metadata_dict[f])\n",
        "\n",
        "ax.set_ylabel('Probabilities')\n",
        "ax.set_yscale('log')\n",
        "ax.set_title('명목변수에 따른 토픽 비율')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(['Topic #{}'.format(k) for k in range(DMR_lee.k)],rotation=45) # x축 라벨 45도로 회전\n",
        "ax.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}